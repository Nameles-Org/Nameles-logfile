#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""Script for migrating log files into postgreSQL tables"""

import os
import re
import sys
sys.path.append("/usr/share/nameles/libs")
import six
import csv
import gzip
import string
import psycopg2
import argparse
from io import StringIO
from tld import get_tld
from time import time
from numpy import array_split
# from getpass import getuser
from datetime import timedelta
from collections import Counter
from multiprocessing import Process, Pool, cpu_count, Value
from psutil import virtual_memory, cpu_count

__author__ = "Antonio Pastor"
__copyright__ = "Copyright 2016, Antonio Pastor"

__license__ = "GPL v3"
__version__ = "1.1"
__maintainer__ = "Antonio Pastor"
__email__ = "anpastor@it.uc3m.es"
__status__ = "Development"


POSTGRES_USER = 'nameles' # getuser()
POSTGRES_DB = 'nameles'
with open('/usr/share/nameles/nameles-port') as port_file:
	POSTGRES_PORT = port_file.readline().rstrip()
TEMP_TABLE = 'temp' # default value changed in main

csv_header = False
ip_field = None
domain_field = None

# The memory used uploading the logs depends directly on these two variables
mem = virtual_memory()

CORES = int(cpu_count()/2) # Number of threads for loading log files
CHUNK_SIZE = int((mem.total/1024/1024/1024)*0.75/CORES) # Number of files that each thread load in memory before inserting the values in the database

processed_files = None
total_files = None

WWW = re.compile('^www\.')
IP_VALID = re.compile('^((25[0-5]|2[0-4][0-9]|[01]?[0-9]{1,2})\.){1,3}(25[0-5]|2[0-4][0-9]|[01]?[0-9]{1,2})$')


def main(args):
    global TEMP_TABLE, processed_files, total_files, csv_header, ip_field, domain_field

    log_day = args.date
    TEMP_TABLE = "tuples.temp_" + log_day
    tuples_table = "tuples.ip_ref_" + log_day
    compressed_files = args.file
    csv_header = args.header
    ip_field = args.ip
    domain_field = args.url

    processed_files = Value('i',0)
    total_files = Value('i',len(compressed_files))

    conn = psycopg2.connect("dbname=%s user=%s port=%s" % (POSTGRES_DB, POSTGRES_USER, POSTGRES_PORT))
    cur = conn.cursor()
    cur.execute("SET ROLE nameles")
    conn.commit()
    cur.execute("CREATE table IF NOT EXISTS {} ( ) INHERITS (tuples.ip_ref)".format(TEMP_TABLE))
    conn.commit()

    printProgress(processed_files.value, total_files.value,
        prefix = '[' + str(processed_files.value) + '/'+str(total_files.value)+']', suffix = '',
        decimals = 0, barLength = 50)
    start_time = time()
    try:
        pool = Pool(processes=CORES, maxtasksperchild=1)
        torun = [compressed_files[i:i+CHUNK_SIZE] for i in range(0,len(compressed_files),CHUNK_SIZE)]
        pool.map(read_gzcsv, torun, 1)
    except:
        print("error on parallel COPY to postgres")        
        print(traceback.format_exc())
        sys.exit(1)
    finally:
        pool.close()
        pool.terminate()
        pool.join()

    printProgress(processed_files.value, total_files.value,
        prefix = '[' + str(processed_files.value) + '/'+str(total_files.value)+']', suffix = '',
        decimals = 0, barLength = 50)
    copied_time = time()
    print("Files copied in {}".format(timedelta(seconds=copied_time-start_time) ) )

    cur.execute("ANALYZE {}".format(TEMP_TABLE))
    conn.commit()
       
    analyze_time = time()
    print("Analyzed temporary table from copy in {}".format(timedelta(seconds=analyze_time-copied_time) ) )

    print("Creating table with (ip, referrer, count) tuples")
    cur.execute("show work_mem")
    wm = cur.fetchall()[0][0]
    cur.execute("SET work_mem = '24GB'")
    conn.commit()
    cur.execute("CREATE table {0} AS (SELECT ip, referrer, sum(cnt) AS cnt FROM {1} GROUP BY ip,referrer)".format(tuples_table,TEMP_TABLE) )
    cur.execute("SET work_mem = '{0}'".format(wm))
    cur.execute("DROP TABLE {0}".format(TEMP_TABLE))
    conn.commit()
    conn.close()
     
    tuples_time = time()
    print("Created tuples table in {}".format(timedelta(seconds=tuples_time-analyze_time) ) )

    # Stats tables processes
    proc_refs = Process(target=compute_domain_score, args=(log_day,tuples_table))
    proc_ips = Process(target=compute_ip_score, args=(log_day,tuples_table))
    proc_ips.start()
    proc_refs.start()

    proc_refs.join()
    proc_ips.join()

    # Create index process
    proc_idx = Process(target=create_indexes, args=(tuples_table,))
    proc_idx.start()
    proc_idx.join()

    final_time = time()
    print("Total time {}".format(timedelta(seconds=final_time-start_time)) )
    sys.exit(0)

def parse_domain(domain):
    domain = domain.lower() # force lowercase to use binary collate in the db
    # app ids come clean
    if domain.startswith('com.'):
        return domain

    domain = ''.join(filter(string.printable.__contains__, domain)) # remove non printable characters
    domain_cleaned = WWW.subn('',domain.replace('"','').strip().split('//')[-1].split('/')[0],count=1)[0]
    # return cleaned top level domain or discard
    try:
        return get_tld("https://" + domain_cleaned )
    except:
        return ''
    
def parse_ip(ip_ascii):
    parsed_ip = ip_ascii.strip('"').split(',')[0].strip(' ')
    # IP_VALID is the regular expression for IPs v4, declared at the beginning of the script 
    if IP_VALID.match(parsed_ip):
        return parsed_ip
    else:    
        return ''

# class LogDialect(csv.Dialect):
#     delimiter = ','
#     doublequote = True
#     scapechar = '"'
#     lineterminator = '\n'
#     quotechar = '"'
#     quoting = csv.QUOTE_MINIMAL
#     skipinitialspace = True
#     strict = True
#     
#     def __init__(self):
#         csv.Dialect.__init__(self)


# Function for processing csv files compressed in gzip format
def read_gzcsv(gzfiles):
    global processed_files, total_files # progress bar variables
    _tuple_counter = Counter()
    if isinstance(gzfiles, six.string_types):
        gzfiles = [gzfiles]
    for gzfile in gzfiles:
        printProgress(processed_files.value, total_files.value,
            prefix = '[' + str(processed_files.value) + '/'+ str(total_files.value)+']', suffix = '',
            decimals = 0, barLength = 50)
        with gzip.open(gzfile,mode='rt', encoding='utf-8') as file:
            _reader = csv.reader(file)
            if csv_header:
                next(_reader)
            for row in _reader:
                _ip = parse_ip(row[ip_field])
                _ref = parse_domain(row[domain_field])
                if _ip!='' and _ref!='':
                    _tuple_counter[(_ip,_ref)] += 1                 
        with processed_files.get_lock():
            processed_files.value += 1
    # end for gzfile in gzfiles

    _buffer = StringIO('\n'.join("{}\t{}\t{}".format(x[0],x[1],v) for x,v in _tuple_counter.items()))
    _conn = psycopg2.connect("dbname=%s user=%s port=%s" % (POSTGRES_DB, POSTGRES_USER, POSTGRES_PORT))
    _cur = _conn.cursor()
    _cur.copy_from(_buffer, '{0}'.format(TEMP_TABLE), sep='\t', size=2**26)
    _conn.commit()
    return

def create_indexes(_table_name):
    _idx0_time = time()
    _conn = psycopg2.connect("dbname=%s user=%s port=%s" % (POSTGRES_DB, POSTGRES_USER, POSTGRES_PORT))
    _cur = _conn.cursor()
    _cur.execute("CREATE INDEX ON {} (ip)".format(_table_name))
    _cur.execute("CREATE INDEX ON {} (referrer)".format(_table_name))
    _cur.execute("ANALYZE {}".format(_table_name))
    _conn.commit()
    _idx1_time = time()
    print("Created tuples indexes in {}".format(timedelta(seconds=_idx1_time-_idx0_time)) )
    return

def compute_domain_score(_log_day,_tuples_table):
    _refs0_time = time()
    _conn = psycopg2.connect("dbname=%s user=%s port=%s" % (POSTGRES_DB, POSTGRES_USER, POSTGRES_PORT))
    _cur = _conn.cursor()
    _cur.execute("SET ROLE nameles")
    _conn.commit()
    print("Creating referrer's stats table")
    _cur.execute("CREATE TABLE stats.referrer_{0} AS (SELECT referrer, sum(cnt) AS total_{0}, norm_entropy(cnt) AS score_{0} FROM {1} GROUP BY referrer)".format(_log_day,_tuples_table))
    _conn.commit()
    _cur.execute("ANALYZE stats.referrer_{0}".format(_log_day))
    _conn.commit()
    _refs1_time = time()
    print("Created referrer's stats table in {}".format(timedelta(seconds=_refs1_time-_refs0_time)) )
    query = """
        BEGIN;

        LOCK TABLE stats.referrer IN EXCLUSIVE MODE;

        ALTER TABLE stats.referrer DROP CONSTRAINT IF EXISTS referrer_pkey;

        ALTER TABLE stats.referrer ADD COLUMN total_{0} int8 default NULL, ADD COLUMN score_{0} int4 default null;
        UPDATE stats.referrer m
        SET (total_{0}, score_{0}) = (d.total_{0}, d.score_{0})
        FROM stats.referrer_{0} d
        WHERE d.total_{0}>=50 and m.referrer=d.referrer;

        INSERT INTO stats.referrer (referrer, total_{0}, score_{0})
        SELECT d.referrer, d.total_{0}, d.score_{0} FROM stats.referrer_{0} d
        LEFT OUTER JOIN stats.referrer m ON (d.referrer = m.referrer)
        WHERE d.total_{0}>=50 and m.referrer IS NULL;

        COMMIT;
        """.format(_log_day)

    print("Merging referrer's stats table")
    _cur.execute(query)
    _conn.commit()
    _cur.execute("ALTER TABLE stats.referrer ADD PRIMARY KEY (referrer);")
    _conn.commit()
    _cur.execute("ANALYZE stats.referrer")
    _cur.execute("DROP TABLE stats.referrer_{0}".format(_log_day))
    _conn.commit()
    _refs2_time = time()
    print("Merged referrer's stats table in {}".format(timedelta(seconds=_refs2_time-_refs1_time)) )
    return

def compute_ip_score(_log_day,_tuples_table):
    _ips0_time = time()
    _conn = psycopg2.connect("dbname=%s user=%s port=%s" % (POSTGRES_DB, POSTGRES_USER, POSTGRES_PORT))
    _cur = _conn.cursor()
    _cur.execute("SET ROLE nameles")
    _conn.commit()
    print("Creating IP's stats table")
    _cur.execute("CREATE TABLE stats.ip_{0} AS (SELECT ip, sum(cnt) AS total_{0}, norm_entropy(cnt) AS score_{0} FROM {1} GROUP BY ip)".format(_log_day,_tuples_table))
    _conn.commit()
    _cur.execute("ANALYZE stats.ip_{0}".format(_log_day))
    _conn.commit()
    _ips1_time = time()
    print("Created IP's stats table in {}".format(timedelta(seconds=_ips1_time-_ips0_time)) )
    query = """
        BEGIN;

        LOCK TABLE stats.ip IN EXCLUSIVE MODE;

        ALTER TABLE stats.ip DROP CONSTRAINT IF EXISTS ip_pkey;

        ALTER TABLE stats.ip ADD COLUMN total_{0} int8 default NULL, ADD COLUMN score_{0} int4 default null;
        UPDATE stats.ip m
        SET (total_{0}, score_{0}) = (d.total_{0}, d.score_{0})
        FROM stats.ip_{0} d
        WHERE d.total_{0}>=50 and d.ip=m.ip;

        INSERT INTO stats.ip (ip, total_{0}, score_{0})
        SELECT d.ip, d.total_{0}, d.score_{0} FROM stats.ip_{0} d
        LEFT OUTER JOIN stats.ip m ON (m.ip = d.ip)
        WHERE d.total_{0}>=50 and m.ip IS NULL;

        COMMIT;
        """.format(_log_day)

    print("Merging IP's stats table")
    _cur.execute(query)
    _conn.commit()
    _cur.execute("ALTER TABLE stats.ip ADD PRIMARY KEY (ip);")
    _conn.commit()
    _cur.execute("ANALYZE stats.ip")
    _cur.execute("DROP TABLE stats.ip_{0}".format(_log_day))
    _conn.commit()
    _ips2_time = time()
    print("Merged IP's stats table in {}".format(timedelta(seconds=_ips2_time-_ips1_time)) )
    return

# Print iterations progress (http://stackoverflow.com/questions/3173320/text-progress-bar-in-the-console)
def printProgress (iteration, total, prefix = '', suffix = '', decimals = 1, barLength = 100):
    """
    Call in a loop to create terminal progress bar
    @params:
        iteration   - Required  : current iteration (Int)
        total       - Required  : total iterations (Int)
        prefix      - Optional  : prefix string (Str)
        suffix      - Optional  : suffix string (Str)
        decimals    - Optional  : positive number of decimals in percent complete (Int)
        barLength   - Optional  : character length of bar (Int)
    """
    formatStr = "{0:." + str(decimals) + "f}"
    percent = formatStr.format(100 * (iteration / float(total)))
    filledLength = int(round(barLength * iteration / float(total)))
    bar = '█' * filledLength + '-' * (barLength - filledLength)
    sys.stdout.write('\r%s |%s| %s%s %s' % (prefix, bar, percent, '%', suffix)),
    if iteration == total:
        sys.stdout.write('\n')
    sys.stdout.flush()
    return


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument('-H', '--header', help='First line in csv is a header line', action='store_true')
    requiredArgs = parser.add_argument_group('required arguments')
    requiredArgs.add_argument('-i', '--ip', help='index of IP field in the csv (0 indexed)', required=True, type=int)
    requiredArgs.add_argument('-u', '--url', help='index of url/domain/app ID field in the csv (0 indexed)', required=True, type=int)
    requiredArgs.add_argument('-d', '--date', help='date of the log files', required=True)
    requiredArgs.add_argument("file", nargs='+', help="compressed file(s) to upload to the database")
    args = parser.parse_args()
    main(args)

